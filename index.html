<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <!-- Mobile viewport and app settings -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="apple-mobile-web-app-title" content="TrashView">
  <!-- Replace with your own app icon URL -->
  <link rel="apple-touch-icon" href="path/to/icon.png">
  <title>TrashView</title>
  
  <!-- Load TensorFlow.js -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
  <!-- Load MediaPipe Hands via hand-pose-detection -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/hand-pose-detection"></script>
  <!-- MediaPipe Hands dependency -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands"></script>
  
  <style>
    body {
      margin: 0;
      background: #222;
      color: white;
      overflow: hidden;
    }
    /* For debugging you can show the video; hide it later by setting display:none */
    #video {
      display: none;
      position: absolute;
      top: 10px;
      left: 10px;
      width: 320px;
      height: 240px;
      border: 2px solid white;
      z-index: 2;
    }
    #canvas {
      position: absolute;
      top: 0;
      left: 0;
      z-index: 1;
    }
  </style>
</head>
<body>
  <video id="video" autoplay muted playsinline></video>
  <canvas id="canvas" width="640" height="480"></canvas>

  <script>
    const video = document.getElementById("video");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");

    let detector, materialClassifier;
    const labels = ["plastic", "metal", "glass", "paper", "organic", "other"];
    const recyclable = ["plastic", "metal", "glass", "paper"];

    // Helper: Compute bounding box from wrist and fingertip keypoints only.
    function getBoundingBox(hand) {
      // Use indices: 0 (wrist), 4, 8, 12, 16, 20 (fingertips)
      const indices = [0, 4, 8, 12, 16, 20];
      const xs = indices.map(i => hand.keypoints[i].x);
      const ys = indices.map(i => hand.keypoints[i].y);
      const xMin = Math.min(...xs);
      const xMax = Math.max(...xs);
      const yMin = Math.min(...ys);
      const yMax = Math.max(...ys);
      return { xMin, yMin, width: xMax - xMin, height: yMax - yMin };
    }

    async function init() {
      try {
        // Start webcam stream.
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        video.srcObject = stream;
        await video.play();
        console.log("Video stream started.");

        // Load the hand detector.
        detector = await handPoseDetection.createDetector(
          handPoseDetection.SupportedModels.MediaPipeHands,
          {
            runtime: 'mediapipe',
            modelType: 'lite',
            solutionPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/hands'
          }
        );
        console.log("Hand detector loaded.");

        // Load the material classifier model.
        materialClassifier = await tf.loadLayersModel("model/model.json");
        console.log("Material classifier loaded.");

        // Start the detection loop.
        detectLoop();
      } catch (err) {
        console.error("Error during initialization:", err);
      }
    }

    async function detectLoop() {
      // Clear canvas and draw the flipped video.
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      ctx.save();
      ctx.translate(canvas.width, 0);
      ctx.scale(-1, 1);
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      ctx.restore();

      try {
        // Estimate hands in the current video frame.
        const hands = await detector.estimateHands(video);
        // Log for debugging.
        console.log("Hands detected:", hands);

        if (hands.length > 0) {
          // Compute bounding box using only wrist and fingertip keypoints.
          const bbox = getBoundingBox(hands[0]);
          // Add padding (in pixels).
          const pad = 20;
          const paddedBox = {
            xMin: bbox.xMin - pad,
            yMin: bbox.yMin - pad,
            width: bbox.width + pad * 2,
            height: bbox.height + pad * 2
          };
          // Flip the x coordinate to match the mirrored canvas.
          const flippedX = canvas.width - (paddedBox.xMin + paddedBox.width);
          const x = flippedX;
          const y = paddedBox.yMin;
          const w = paddedBox.width;
          const h = paddedBox.height;
          
          // Draw the padded hand bounding box (yellow).
          ctx.strokeStyle = "yellow";
          ctx.lineWidth = 2;
          ctx.strokeRect(x, y, w, h);

          // Define a crop region for classification; you can adjust this as needed.
          // Here we crop the padded bounding box directly.
          const cropX = x;
          const cropY = y;
          const cropW = w;
          const cropH = h;
          
          // Draw crop region (red) for visualization.
          ctx.strokeStyle = "red";
          ctx.strokeRect(cropX, cropY, cropW, cropH);

          // Extract image data from the crop region.
          let imgData;
          try {
            imgData = ctx.getImageData(cropX, cropY, cropW, cropH);
          } catch (e) {
            console.error("Error extracting image data:", e);
            imgData = null;
          }

          if (imgData) {
            const inputTensor = tf.browser.fromPixels(imgData)
              .resizeBilinear([384, 384])
              .toFloat()
              .div(tf.scalar(255))
              .expandDims();

            const prediction = materialClassifier.predict(inputTensor);
            const probs = prediction.dataSync();
            const idx = prediction.argMax(-1).dataSync()[0];
            const confidence = probs[idx];
            const label = labels[idx];

            // Display prediction text.
            ctx.fillStyle = "white";
            ctx.font = "16px Arial";
            ctx.fillText(`${label} (${(confidence * 100).toFixed(1)}%)`, cropX, cropY - 5);

            // Draw an arrow toward a bin based on material type.
            const target = recyclable.includes(label)
              ? { x: canvas.width - 50, y: canvas.height - 50 }
              : { x: 50, y: canvas.height - 50 };
            ctx.beginPath();
            ctx.moveTo(cropX + cropW / 2, cropY + cropH / 2);
            ctx.lineTo(target.x, target.y);
            ctx.strokeStyle = "lime";
            ctx.lineWidth = 2;
            ctx.stroke();
          }
        }
      } catch (err) {
        console.error("Detection error:", err);
      }

      requestAnimationFrame(detectLoop);
    }

    init();
  </script>
</body>
</html>
