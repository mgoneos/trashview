<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Hand Detection and Material Classification</title>
  <!-- Load TensorFlow.js and the Handpose model -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose"></script>
  <style>
    body {
      margin: 0;
      padding: 0;
      background-color: #222;
      color: #eee;
      text-align: center;
    }
    #video, #canvas {
      position: absolute;
      left: 50%;
      top: 50%;
      transform: translate(-50%, -50%);
    }
    #canvas {
      z-index: 10;
    }
  </style>
</head>
<body>
  <!-- Video stream element -->
  <video id="video" width="640" height="480" autoplay playsinline muted></video>
  <!-- Canvas for drawing overlays, bounding boxes, and the red line -->
  <canvas id="canvas" width="640" height="480"></canvas>
  
  <script>
    const video = document.getElementById("video");
    const canvas = document.getElementById("canvas");
    const context = canvas.getContext("2d");

    // Fixed positions for bins on the canvas.
    const trashBinPos = { x: 50, y: canvas.height - 50 };
    const recycleBinPos = { x: canvas.width - 50, y: canvas.height - 50 };

    // List of materials considered recyclable.
    const recyclableMaterials = ["plastic", "metal", "glass", "paper"];

    // Global variables for our models.
    let handposeModel;       // For hand detection.
    let materialClassifier;  // Your ResNet50-based material classification model.

    // Load the Handpose model.
    handpose.load().then(model => {
      handposeModel = model;
      console.log("Handpose model loaded.");
    }).catch(err => console.error("Error loading handpose model:", err));

    // Load your material classifier model (converted from your .keras ResNet50).
    tf.loadLayersModel("model/model.json")
      .then(model => {
        materialClassifier = model;
        console.log("Material classifier (ResNet50) loaded.");
      })
      .catch(err => console.error("Error loading material classifier:", err));

    // Start the webcam stream.
    function startVideoStream() {
      if (navigator.mediaDevices.getUserMedia) {
        navigator.mediaDevices.getUserMedia({ video: true })
          .then(stream => {
            video.srcObject = stream;
          })
          .catch(err => console.error("Error accessing webcam:", err));
      } else {
        alert("getUserMedia is not supported by your browser.");
      }
    }

    // Draw the fixed bin areas.
    function drawBins() {
      // Draw Trash bin (left).
      context.fillStyle = "gray";
      context.beginPath();
      context.arc(trashBinPos.x, trashBinPos.y, 30, 0, 2 * Math.PI);
      context.fill();
      context.font = "16px Arial";
      context.fillStyle = "white";
      context.fillText("Trash", trashBinPos.x - 20, trashBinPos.y + 5);
  
      // Draw Recycling bin (right).
      context.fillStyle = "blue";
      context.beginPath();
      context.arc(recycleBinPos.x, recycleBinPos.y, 30, 0, 2 * Math.PI);
      context.fill();
      context.fillStyle = "white";
      context.fillText("Recycle", recycleBinPos.x - 30, recycleBinPos.y + 5);
    }

    // Asynchronously classify the material using your ResNet50 model.
    async function classifyMaterial(imageData) {
      // Convert ImageData to a tensor.
      let tensor = tf.browser.fromPixels(imageData);
      // Resize tensor to match the model's expected input size (384x384 in this example).
      tensor = tf.image.resizeBilinear(tensor, [384, 384]);
      // Expand dimensions to create a batch (resulting shape: [1, 384, 384, 3]).
      tensor = tensor.expandDims(0);
      // Normalize pixel values to [0, 1].
      tensor = tensor.toFloat().div(tf.scalar(255));
  
      if (materialClassifier) {
        const prediction = materialClassifier.predict(tensor);
        // Use argMax to retrieve the index of the highest score.
        const predictedIndex = prediction.argMax(-1).dataSync()[0];
        // Map the index to material labels (adjust as per your training).
        const labels = ["plastic", "metal", "glass", "paper", "organic", "other"];
        return labels[predictedIndex];
      } else {
        return "unknown";
      }
    }
  
    // Main loop: detect hand, compute bounding box, classify the region, and draw overlays.
    async function predictWebcam() {
      // Draw the current video frame on the canvas.
      context.drawImage(video, 0, 0, canvas.width, canvas.height);
      drawBins();
  
      if (handposeModel) {
        // Run hand detection.
        const hands = await handposeModel.estimateHands(video);
  
        if (hands.length > 0) {
          // Use the first detected hand.
          const hand = hands[0];
          const landmarks = hand.landmarks; // Array of 21 [x, y, z] coordinates.
  
          // Compute the bounding box from hand landmarks.
          let minX = Infinity, minY = Infinity, maxX = -Infinity, maxY = -Infinity;
          landmarks.forEach(point => {
            const [x, y] = point;
            if (x < minX) minX = x;
            if (y < minY) minY = y;
            if (x > maxX) maxX = x;
            if (y > maxY) maxY = y;
          });
  
          // Add padding to the bounding box.
          const pad = 20;
          minX = Math.max(0, minX - pad);
          minY = Math.max(0, minY - pad);
          maxX = Math.min(canvas.width, maxX + pad);
          maxY = Math.min(canvas.height, maxY + pad);
          const boxWidth = maxX - minX;
          const boxHeight = maxY - minY;
  
          // Draw the bounding box around the hand.
          context.strokeStyle = "#00FFFF";
          context.lineWidth = 2;
          context.strokeRect(minX, minY, boxWidth, boxHeight);
  
          // Extract the region of interest (ROI) for material classification.
          let roiImageData;
          try {
            roiImageData = context.getImageData(minX, minY, boxWidth, boxHeight);
          } catch (e) {
            console.error("Error extracting image data:", e);
          }
  
          if (roiImageData) {
            // Run material classification on the extracted region.
            const material = await classifyMaterial(roiImageData);
            console.log("Predicted material:", material);
  
            // Determine target bin based on the predicted material.
            const target = recyclableMaterials.includes(material)
                            ? recycleBinPos
                            : trashBinPos;
  
            // Determine the center of the hand bounding box.
            const centerX = minX + boxWidth / 2;
            const centerY = minY + boxHeight / 2;
  
            // Draw a red line from the hand's center to the target bin.
            context.strokeStyle = "red";
            context.lineWidth = 3;
            context.beginPath();
            context.moveTo(centerX, centerY);
            context.lineTo(target.x, target.y);
            context.stroke();
  
            // Optionally display the predicted material above the bounding box.
            context.font = "18px Arial";
            context.fillStyle = "red";
            context.fillText(material, minX, (minY > 20 ? minY - 5 : 20));
          }
        }
      }
      requestAnimationFrame(predictWebcam);
    }
  
    // Start the video stream on page load.
    startVideoStream();
    // Start the prediction loop once the video is ready.
    video.addEventListener("loadeddata", predictWebcam);
  </script>
</body>
</html>
