<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Hand & Object Detection with Material Classification</title>
  <!-- Load TensorFlow.js and the Handpose model -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose"></script>
  <style>
    body {
      margin: 0;
      padding: 0;
      background-color: #222;
      color: #eee;
      text-align: center;
    }
    #video, #canvas {
      position: absolute;
      left: 50%;
      top: 50%;
      transform: translate(-50%, -50%);
    }
    #canvas {
      z-index: 10;
    }
  </style>
</head>
<body>
  <!-- Video element for the webcam stream -->
  <video id="video" width="640" height="480" autoplay playsinline muted></video>
  <!-- Canvas for drawing overlays -->
  <canvas id="canvas" width="640" height="480"></canvas>
  
  <script>
    const video = document.getElementById("video");
    const canvas = document.getElementById("canvas");
    const context = canvas.getContext("2d");

    // Fixed bin positions on the canvas.
    const trashBinPos = { x: 50, y: canvas.height - 50 };
    const recycleBinPos = { x: canvas.width - 50, y: canvas.height - 50 };
    // Define recyclable materials.
    const recyclableMaterials = ["plastic", "metal", "glass", "paper"];

    // Global model variables.
    let handposeModel;       // For hand detection.
    let materialClassifier;  // Your ResNet50-based material classification model.

    // Load the Handpose model.
    handpose.load().then(model => {
      handposeModel = model;
      console.log("Handpose model loaded.");
    }).catch(err => console.error("Error loading handpose model:", err));

    // Load your ResNet50 model (converted to TFJS format and hosted locally in folder 'model').
    tf.loadLayersModel("model/model.json")
      .then(model => {
        materialClassifier = model;
        console.log("Material classifier (ResNet50) loaded.");
      })
      .catch(err => console.error("Error loading material classifier:", err));

    // Start the webcam video stream.
    function startVideoStream() {
      if (navigator.mediaDevices.getUserMedia) {
        navigator.mediaDevices.getUserMedia({ video: true })
          .then(stream => { video.srcObject = stream; })
          .catch(err => { console.error("Error accessing webcam:", err); });
      } else {
        alert("getUserMedia is not supported by your browser.");
      }
    }

    // Draw the fixed bins.
    function drawBins() {
      // Trash bin (left)
      context.fillStyle = "gray";
      context.beginPath();
      context.arc(trashBinPos.x, trashBinPos.y, 30, 0, 2 * Math.PI);
      context.fill();
      context.font = "16px Arial";
      context.fillStyle = "white";
      context.fillText("Trash", trashBinPos.x - 20, trashBinPos.y + 5);

      // Recycling bin (right)
      context.fillStyle = "blue";
      context.beginPath();
      context.arc(recycleBinPos.x, recycleBinPos.y, 30, 0, 2 * Math.PI);
      context.fill();
      context.fillStyle = "white";
      context.fillText("Recycle", recycleBinPos.x - 30, recycleBinPos.y + 5);
    }

    // Asynchronously classify material using your ResNet50-based model.
    async function classifyMaterial(imageData) {
      let tensor = tf.browser.fromPixels(imageData);
      // Resize to match your model's expected input (384x384 in this example).
      tensor = tf.image.resizeBilinear(tensor, [384, 384]);
      tensor = tensor.expandDims(0);
      tensor = tensor.toFloat().div(tf.scalar(255));
  
      if (materialClassifier) {
        const prediction = materialClassifier.predict(tensor);
        // Get the index of the highest score.
        const predictedIndex = prediction.argMax(-1).dataSync()[0];
        // Map the index to material labels (adjust this ordering to match your training).
        const labels = ["plastic", "metal", "glass", "paper", "organic", "other"];
        return labels[predictedIndex];
      } else {
        return "unknown";
      }
    }
  
    // Main loop: detect hand, compute ROI for the held object, classify, and draw overlays.
    async function predictWebcam() {
      // Draw the current video frame.
      context.drawImage(video, 0, 0, canvas.width, canvas.height);
      drawBins();
  
      if (handposeModel) {
        // Estimate hand landmarks.
        const hands = await handposeModel.estimateHands(video);
        if (hands.length > 0) {
          const hand = hands[0];
          const landmarks = hand.landmarks;  // Array of 21 points [x,y,z]
          
          // Compute the hand bounding box (for reference).
          let minX = Infinity, minY = Infinity, maxX = -Infinity, maxY = -Infinity;
          landmarks.forEach(pt => {
            const [x, y] = pt;
            if (x < minX) minX = x;
            if (y < minY) minY = y;
            if (x > maxX) maxX = x;
            if (y > maxY) maxY = y;
          });
  
          // Determine key landmarks: wrist (landmarks[0]) and index finger tip (landmarks[8]).
          const wrist = landmarks[0];
          const indexTip = landmarks[8];
  
          // Compute the vector from wrist to index finger tip.
          let vx = indexTip[0] - wrist[0];
          let vy = indexTip[1] - wrist[1];
          const vectorLength = Math.sqrt(vx * vx + vy * vy);
          let nx = 0, ny = 0;
          if (vectorLength > 0) {
            nx = vx / vectorLength;
            ny = vy / vectorLength;
          }
  
          // Instead of classifying the hand region, shift the ROI toward the object.
          // We'll start from the index finger tip and move further in that direction.
          // Use half the hand box height as an offset.
          const boxHeight = maxY - minY;
          const offset = 0.5 * boxHeight;  // adjust multiplier as needed
          const roiCenterX = indexTip[0] + nx * offset;
          const roiCenterY = indexTip[1] + ny * offset;
  
          // Define the ROI size (e.g., twice the hand bounding box size).
          const boxWidth = maxX - minX;
          let roiWidth = boxWidth * 2;
          let roiHeight = boxHeight * 2;
  
          // Calculate top-left corner from the ROI center.
          let roiX = roiCenterX - roiWidth / 2;
          let roiY = roiCenterY - roiHeight / 2;
  
          // Clamp the ROI within the canvas boundaries.
          roiX = Math.max(0, roiX);
          roiY = Math.max(0, roiY);
          if (roiX + roiWidth > canvas.width) roiWidth = canvas.width - roiX;
          if (roiY + roiHeight > canvas.height) roiHeight = canvas.height - roiY;
  
          // Draw the ROI bounding box (in yellow) to visualize the target object region.
          context.strokeStyle = "#FFFF00";
          context.lineWidth = 2;
          context.strokeRect(roiX, roiY, roiWidth, roiHeight);
  
          // Extract the ROI image data.
          let roiImageData;
          try {
            roiImageData = context.getImageData(roiX, roiY, roiWidth, roiHeight);
          } catch (e) {
            console.error("Error extracting ROI image data:", e);
          }
  
          if (roiImageData) {
            // Classify the material in the ROI.
            const material = await classifyMaterial(roiImageData);
            console.log("Predicted material:", material);
  
            // Decide the target bin.
            const target = recyclableMaterials.includes(material)
              ? recycleBinPos
              : trashBinPos;
  
            // Compute the ROI center.
            const roiCenterCalcX = roiX + roiWidth / 2;
            const roiCenterCalcY = roiY + roiHeight / 2;
  
            // Draw a red line from the ROI center to the target bin.
            context.strokeStyle = "red";
            context.lineWidth = 3;
            context.beginPath();
            context.moveTo(roiCenterCalcX, roiCenterCalcY);
            context.lineTo(target.x, target.y);
            context.stroke();
  
            // Optionally display the predicted material near the ROI.
            context.font = "18px Arial";
            context.fillStyle = "red";
            context.fillText(material, roiX, (roiY > 20 ? roiY - 5 : 20));
          }
        }
      }
      requestAnimationFrame(predictWebcam);
    }
  
    // Start video stream and the prediction loop.
    startVideoStream();
    video.addEventListener("loadeddata", predictWebcam);
  </script>
</body>
</html>
